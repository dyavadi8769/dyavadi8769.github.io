#### Disaster Classification & Severity Assessment

To evaluate the systemâ€™s ability to classify disaster types and assess severity, we conducted controlled tests using a diverse set of historical weather data sets enriched with ground-truth labels. The LLM (Gemini 1.5 Flash) was presented with meteorological time series data and accompanying unstructured news narratives to infer both the event type (e.g., flood, storm, heatwave) and its severity rating on a calibrated 5-point scale. Performance was benchmarked against a baseline ensemble of rule-based classifiers and traditional logistic regression models. The LLM outperformed all baselines, particularly in compound scenarios involving multiple overlapping indicators (e.g., storm + power outage). Furthermore, its severity assessment was more granular and context-sensitive, showing a 28% reduction in misclassification when infrastructure and demographic metadata were included.

#### Simulated Environment

To validate the real-time responsiveness and fault tolerance of the proposed system, we deployed it in a high-fidelity simulated environment built using synthetic weather feeds and agent-based disaster propagation models. Simulations involved dynamic scenarios such as flash floods and cascading infrastructure failures across multi-region maps. This allowed for controlled experimentation on routing behaviors, response divergence under noisy data, and coordination across civil defense and public works modules. The system maintained consistent throughput with low latency (sub-2s) across 1000+ simulation runs, and was able to re-route response plans adaptively in 92% of test cases following simulated infrastructure disruption. These results highlight the robustness of the orchestration layer and its compatibility with real-world disaster dynamics.

#### Response Time & Accuracy

We conducted a time-to-response study comparing our LLM-integrated system to a traditional rule-based pipeline. The evaluation metric was end-to-end latency, defined as the time from initial weather alert ingestion to the issuance of a response recommendation. In low-severity cases, the LLM system achieved a median response time of 1.8 seconds, while maintaining high classification fidelity. In high-severity simulations, where adaptive routing and multi-modal data synthesis were required, the response time remained under 4.2 seconds on average. Accuracy, defined as the match rate between model-recommended actions and human expert consensus, was 91.4%, significantly higher than the baseline (76.2%). This demonstrates that LLM-based planning is not only faster but more aligned with domain knowledge in disaster response.

#### Social Media Integration & Validation

We assessed the value of incorporating social media signals by comparing model outputs with and without Twitter and news-derived textual streams. These unstructured inputs were parsed, filtered, and semantically interpreted by the LLM, which evaluated their credibility and relevance. Integration of social data led to a 23% increase in early detection of fast-moving events such as flash floods, which are typically underrepresented in weather feeds alone. Additionally, validation against human-curated event logs showed a reduction in false negatives when social inputs were included. Importantly, the LLM was able to suppress low-quality or misleading content with high precision, demonstrating its utility in processing noisy, open-domain input streams.

#### Impact of Human Approval

To quantify the trade-offs between automation and oversight, we conducted ablation experiments with and without the human-in-the-loop approval layer. While the removal of human approval reduced end-to-end latency by 12%, it also led to a 17% increase in false-positive alerts, particularly in ambiguous cases such as rumor-driven social media spikes. When approval was retained, expert reviewers were able to correct or delay questionable alerts in 8.5% of cases, enhancing trust and avoiding unnecessary mobilization. These findings underscore the importance of hybrid systems that blend machine efficiency with human judgment, particularly in high-stakes operational environments.


